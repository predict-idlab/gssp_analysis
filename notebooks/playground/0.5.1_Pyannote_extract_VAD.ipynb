{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import torch\n",
    "import noisereduce as nr\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from sgs_utils.path_conf import loc_data_dir, interim_speech_data_dir\n",
    "from sgs_utils.data_filtering import get_valid_audio_mask\n",
    "from sgs_utils.dataframes import groupby_consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session = pd.read_parquet(loc_data_dir.joinpath(\"df_session_tot_cleaned.parquet\"))\n",
    "df_session_v = df_session[get_valid_audio_mask(df_session) & (df_session.wav_duration_s > 16.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract VAD on full duration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**: we use the normalized full-duration 16kHz numpy representations of the wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_full_VADs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8b4d0951eb4d329551d9583a4e55b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not extracted_full_VADs:\n",
    "    from pyannote.audio import Pipeline\n",
    "\n",
    "    # load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\")\n",
    "\n",
    "    # the audio files are processed via pipeline._segementation\n",
    "    df_list = []\n",
    "    for _, r in tqdm(df_session_v.iterrows(), total=len(df_session_v)):\n",
    "        try:\n",
    "\n",
    "            if r.wav_duration_s < 17:\n",
    "                continue\n",
    "\n",
    "            npy_file = list(\n",
    "                interim_speech_data_dir.glob(\n",
    "                    f\"full_dur_16khz_norm/*{r.ID}/{r.DB}/{r.pic_name}*{r.time_str}*.npy\"\n",
    "                )\n",
    "            )[0]\n",
    "            \n",
    "            arr = torch.from_numpy(np.load(npy_file))\n",
    "            out = pipeline._segmentation({'waveform': arr, 'sample_rate': 16_000})\n",
    "            probas = out.data.ravel()\n",
    "            sliding_window = out.sliding_window\n",
    "            s = pd.Series(\n",
    "                probas,\n",
    "                index=np.arange(\n",
    "                    start=sliding_window.start + sliding_window.duration,\n",
    "                    step=sliding_window.step,\n",
    "                    stop=sliding_window.start\n",
    "                    + sliding_window.duration\n",
    "                    + sliding_window.step * probas.shape[0],\n",
    "                    dtype=\"float64\",\n",
    "                )[: probas.shape[0]],\n",
    "                name=\"voice_proba\",\n",
    "            )\n",
    "\n",
    "\n",
    "            # slice the series to only retain 15 second data until the penultimate second\n",
    "            # Note: this is the same method as with the \n",
    "            # t_end = s.index[-1]\n",
    "            # s = s[max(1, t_end - 15 - 1): t_end - 1]\n",
    "\n",
    "            s.index.name = \"time_s\"\n",
    "            s = s.reset_index(drop=False)\n",
    "\n",
    "            s[\"pic_name\"] = npy_file.name.split(\"__\")[0]\n",
    "            s[\"time_str\"] = npy_file.name.split(\"__\")[1].split(\".\")[0]\n",
    "            s[\"DB\"] = npy_file.parent.name\n",
    "            s['sw_duration'] = sliding_window.duration\n",
    "            s['sw_step'] = sliding_window.step\n",
    "            s[\"ID\"] = npy_file.parent.parent.name.split(\"__\")[-1]\n",
    "\n",
    "            df_list.append(s)\n",
    "        except KeyboardInterrupt as e:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    df_vad = pd.concat(df_list)\n",
    "    df_vad['pic_name'] = df_vad['pic_name'].astype('category')\n",
    "    df_vad['DB'] = df_vad['DB'].astype('category')\n",
    "    df_vad['ID'] = df_vad['ID'].astype('category')\n",
    "    df_vad['time_s'] = df_vad['time_s'].astype('float32')\n",
    "\n",
    "    df_vad.to_parquet(loc_data_dir.joinpath('df_vad.parquet'))\n",
    "else:\n",
    "    df_vad = pd.read_parquet(loc_data_dir.joinpath('df_vad.parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply stationary `noisereduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/SPS/interim/speech_webapp/full_dur_16khz_norm/fb807c80-22cc-4964-81a0-8c732f572004/PiSCES/Picture 87__16:45:23.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m fr \u001b[38;5;241m=\u001b[39m FigureResampler(make_subplots(rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shared_xaxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     10\u001b[0m fr\u001b[38;5;241m.\u001b[39mupdate_layout(height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/semi-guided-speech-27YL4uf1-py3.8/lib/python3.8/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/SPS/interim/speech_webapp/full_dur_16khz_norm/fb807c80-22cc-4964-81a0-8c732f572004/PiSCES/Picture 87__16:45:23.npy'"
     ]
    }
   ],
   "source": [
    "# Audio normalization testing code\n",
    "\n",
    "from plotly_resampler import FigureResampler\n",
    "from plotly.subplots import make_subplots\n",
    "import librosa\n",
    "\n",
    "arr = np.load(npy_file)\n",
    "\n",
    "fr = FigureResampler(make_subplots(rows=3, shared_xaxes=True))\n",
    "fr.update_layout(height=800)\n",
    "fr.add_trace({'name': 'original'}, hf_y=arr.ravel())\n",
    "fr.add_trace({'name': 'noise-red'}, hf_y=nr.reduce_noise(arr.ravel(), sr=16_000, stationary=True), col=1, row=2)\n",
    "fr.add_trace({'name': 'noise-red-norm'}, hf_y=librosa.util.normalize(nr.reduce_noise(arr.ravel(), sr=16_000, stationary=True)), col=1, row=3)\n",
    "fr.show(renderer=\"png\", width=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0edaeaf75024f7ab85a1e00f32733f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "# noise reduction\n",
    "if not extracted_full_VADs:\n",
    "    from pyannote.audio import Pipeline\n",
    "\n",
    "    # load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\")\n",
    "\n",
    "    # the audio files are processed via pipeline._segementation\n",
    "    df_list = []\n",
    "    for _, r in tqdm(df_session_v.iterrows(), total=len(df_session_v)):\n",
    "        try:\n",
    "\n",
    "            if r.wav_duration_s < 17:\n",
    "                continue\n",
    "\n",
    "            npy_file = list(\n",
    "                interim_speech_data_dir.glob(\n",
    "                    f\"full_dur_16khz_norm/*{r.ID}/{r.DB}/{r.pic_name}*{r.time_str}*.npy\"\n",
    "                )\n",
    "            )[0]\n",
    "            \n",
    "            arr = torch.from_numpy(\n",
    "                nr.reduce_noise(y=np.load(npy_file), sr=16_000, stationary=True)\n",
    "            )\n",
    "            out = pipeline._segmentation({'waveform': arr, 'sample_rate': 16_000})\n",
    "            probas = out.data.ravel()\n",
    "            sliding_window = out.sliding_window\n",
    "            s = pd.Series(\n",
    "                probas,\n",
    "                index=np.arange(\n",
    "                    start=sliding_window.start + sliding_window.duration,\n",
    "                    step=sliding_window.step,\n",
    "                    stop=sliding_window.start\n",
    "                    + sliding_window.duration\n",
    "                    + sliding_window.step * probas.shape[0],\n",
    "                    dtype=\"float64\",\n",
    "                )[: probas.shape[0]],\n",
    "                name=\"voice_proba\",\n",
    "            )\n",
    "\n",
    "\n",
    "            # slice the series to only retain 15 second data until the penultimate second\n",
    "            # Note: this is the same method as with the \n",
    "            # t_end = s.index[-1]\n",
    "            # s = s[max(1, t_end - 15 - 1): t_end - 1]\n",
    "\n",
    "            s.index.name = \"time_s\"\n",
    "            s = s.reset_index(drop=False)\n",
    "\n",
    "            s[\"pic_name\"] = npy_file.name.split(\"__\")[0]\n",
    "            s[\"time_str\"] = npy_file.name.split(\"__\")[1].split(\".\")[0]\n",
    "            s[\"DB\"] = npy_file.parent.name\n",
    "            s['sw_duration'] = sliding_window.duration\n",
    "            s['sw_step'] = sliding_window.step\n",
    "            s[\"ID\"] = npy_file.parent.parent.name.split(\"__\")[-1]\n",
    "\n",
    "            df_list.append(s)\n",
    "        except KeyboardInterrupt as e:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    df_vad = pd.concat(df_list)\n",
    "    df_vad['pic_name'] = df_vad['pic_name'].astype('category')\n",
    "    df_vad['DB'] = df_vad['DB'].astype('category')\n",
    "    df_vad['ID'] = df_vad['ID'].astype('category')\n",
    "    df_vad['time_s'] = df_vad['time_s'].astype('float32')\n",
    "\n",
    "    df_vad.to_parquet(loc_data_dir.joinpath('df_vad_nr.parquet'))\n",
    "else:\n",
    "    df_vad = pd.read_parquet(loc_data_dir.joinpath('df_vad_nr.parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract VAD on fixed duration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67da9c8d774943a8908b1159770c0f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not extracted_VADs:\n",
    "    from pyannote.audio import Pipeline\n",
    "\n",
    "    # load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\")\n",
    "\n",
    "    # the audio files are processed via pipeline._segementation\n",
    "    df_list = []\n",
    "    for _, r in tqdm(df_session_v.iterrows(), total=len(df_session_v)):\n",
    "        try:\n",
    "\n",
    "            if r.wav_duration_s < 17:\n",
    "                continue\n",
    "\n",
    "            npy_file = list(\n",
    "                speech_data_session_dir.glob(\n",
    "                    f\"*{r.ID}/{r.DB}/{r.pic_name}*{r.time_str}*.wav\"\n",
    "                )\n",
    "            )[0]\n",
    "\n",
    "            out = pipeline._segmentation(npy_file)\n",
    "            probas = out.data.ravel()\n",
    "            sliding_window = out.sliding_window\n",
    "            s = pd.Series(\n",
    "                probas,\n",
    "                index=np.arange(\n",
    "                    start=sliding_window.start + sliding_window.duration,\n",
    "                    step=sliding_window.step,\n",
    "                    stop=sliding_window.start\n",
    "                    + sliding_window.duration\n",
    "                    + sliding_window.step * probas.shape[0],\n",
    "                    dtype=\"float64\",\n",
    "                )[: probas.shape[0]],\n",
    "                name=\"voice_proba\",\n",
    "            )\n",
    "\n",
    "\n",
    "            # slice the series to only retain 15 second data until the penultimate second\n",
    "            # Note: this is the same method as with the \n",
    "            t_end = s.index[-1]\n",
    "            s = s[max(1, t_end - 15 - 1): t_end - 1]\n",
    "\n",
    "            s.index.name = \"time_s\"\n",
    "            s = s.reset_index(drop=False)\n",
    "\n",
    "            s[\"pic_name\"] = npy_file.name.split(\"__\")[0]\n",
    "            s[\"time_str\"] = npy_file.name.split(\"__\")[1].split(\".\")[0]\n",
    "            s[\"DB\"] = npy_file.parent.name\n",
    "            s['sw_duration'] = sliding_window.duration\n",
    "            s['sw_step'] = sliding_window.step\n",
    "            s[\"ID\"] = npy_file.parent.parent.name.split(\"__\")[-1]\n",
    "\n",
    "            df_list.append(s)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    df_vad = pd.concat(df_list)\n",
    "    df_vad['pic_name'] = df_vad['pic_name'].astype('category')\n",
    "    df_vad['DB'] = df_vad['DB'].astype('category')\n",
    "    df_vad['ID'] = df_vad['ID'].astype('category')\n",
    "    df_vad['time_s'] = df_vad['time_s'].astype('float32')\n",
    "\n",
    "    df_vad.to_parquet(loc_data_dir.joinpath('df_vad_fixed_dur.parquet'))\n",
    "else:\n",
    "    df_vad = pd.read_parquet(loc_data_dir.joinpath('df_vad_fixed_dur.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889    3015\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the unique time_str & ID combination\n",
    "ids = df_vad.groupby([\"time_str\", \"ID\"]).size()\n",
    "ids = ids[ids > 0]\n",
    "display(ids.value_counts())\n",
    "ids = ids.reset_index()[['time_str', \"ID\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Construct features from the fixed duration VAD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e5243a9ed247ff8dc2bc4c3d6cdfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_dict = []\n",
    "\n",
    "for _, r in tqdm(ids.iterrows(), total=len(ids)):\n",
    "    mask = (df_vad.ID == r.ID) & (df_vad.time_str == r.time_str)\n",
    "    df_vad_rec = df_vad[mask]\n",
    "    gc = groupby_consecutive(df_vad_rec['voice_proba'] > .5)\n",
    "    gc['duration_s'] = (gc.end - gc.start) * df_vad_rec.sw_duration.iloc[0]\n",
    "\n",
    "    n_speaks = len(gc[gc.voice_proba == True])\n",
    "    n_silences = len(gc[gc.voice_proba == False])\n",
    "\n",
    "    feat_dict.append(\n",
    "        {\n",
    "            \"ID\": r.ID,\n",
    "            \"time_str\": r.time_str,\n",
    "            \"mean_voice_proba\": df_vad_rec['voice_proba'].mean(),\n",
    "            \"speak_max_s\" : gc[gc.voice_proba == True]['duration_s'].max(),\n",
    "            \"speak_mean_s\" : gc[gc.voice_proba == True]['duration_s'].mean(),\n",
    "            \"speak_std_s\": gc[gc.voice_proba == True]['duration_s'].std() if n_speaks > 1 else 0,\n",
    "            \"n_silences\": n_silences,\n",
    "            \"silence_max_s\": gc[gc.voice_proba == False]['duration_s'].max() if n_silences > 0 else 0,\n",
    "            \"silence_mean_s\": gc[gc.voice_proba == False]['duration_s'].mean() if n_silences > 0 else 0,\n",
    "            \"silence_std_s\": gc[gc.voice_proba == False]['duration_s'].std() if n_silences > 1 else 0\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(feat_dict).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.to_parquet(loc_data_dir.joinpath('df_vad_feat.parquet'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "14accab11a8dde1829a82d5477aee5050c56a20d58c0fbbbf6574e6407175af4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
