{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks extracts voicing-related `openSMILE` low-level-descriptors (LLD's) on the parsed (i.e, normalized and converted to 16KhZ mono, see [this notebook](0.4_Parse_Audio_Data.ipynb)) audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import opensmile\n",
    "from multiprocessing import Pool\n",
    "import traceback\n",
    "from typing import Tuple\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from sgs_utils.path_conf import loc_data_dir, interim_speech_data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we utilize the cleaned parquet file obtained by running the 0.1_EDA notebook\n",
    "df_session = pd.read_parquet(loc_data_dir.joinpath(\"df_session_tot_cleaned.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The emobase LLD withholds the \"voiceProb_sma\" feature\n",
    "lld_emobase = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.emobase,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    ")\n",
    "\n",
    "# The compare LLD withholds the \"voicingFinalUnclipped_sma\" feature\n",
    "lld_compare = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Whole file duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f96571220d42cebc5a426b94ba3d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _extract_parse_smile_df(s: opensmile.Smile, f: Path) -> pd.DataFrame:\n",
    "    df_feat = s.process_signal(np.load(f), sampling_rate=16_000, file=f)\n",
    "    df_feat = df_feat.reset_index(drop=False)\n",
    "    df_feat[\"file\"] = df_feat[\"file\"].astype(\"str\")\n",
    "\n",
    "    # df_feat[\"fileName\"] = f.name\n",
    "    df_feat[\"pic_name\"] = f.name.split(\"__\")[0]\n",
    "    df_feat[\"time_str\"] = f.name.split(\"__\")[1].split(\".\")[0]\n",
    "    df_feat[\"DB\"] = f.parent.name\n",
    "    df_feat[\"ID\"] = f.parent.parent.name.split(\"__\")[-1]\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "def _extract_opensmile_f(file: Path) -> Tuple[pd.DataFrame, ...]:\n",
    "    # calculate the global utterance features\n",
    "    return (\n",
    "        _extract_parse_smile_df(lld_emobase, f=file),\n",
    "        _extract_parse_smile_df(lld_compare, f=file),\n",
    "    )\n",
    "\n",
    "\n",
    "out = None\n",
    "with Pool(processes=6) as pool:\n",
    "    # NOTE how we use here the parsed numpy files\n",
    "    npy_files = list(interim_speech_data_dir.glob(\"full_dur_16khz_norm/*/*/*.npy\"))\n",
    "    results = pool.imap_unordered(_extract_opensmile_f, npy_files)\n",
    "    results = tqdm(results, total=len(npy_files))\n",
    "    try:\n",
    "        out = [f for f in results]\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        pool.terminate()\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "\n",
    "def _parse_concat_df(df_conc: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_conc[\"DB\"] = df_conc[\"DB\"].astype(\"category\")\n",
    "    df_conc[\"pic_name\"] = df_conc[\"pic_name\"].astype(\"category\")\n",
    "    df_conc[\"ID\"] = df_conc[\"ID\"].astype(\"category\")\n",
    "    return df_conc\n",
    "\n",
    "\n",
    "df_emobase_lld = _parse_concat_df(pd.concat([o[0] for o in out], ignore_index=True))\n",
    "df_compare_lld = _parse_concat_df(pd.concat([o[1] for o in out], ignore_index=True))\n",
    "\n",
    "del (\n",
    "    out,\n",
    "    _extract_opensmile_f,\n",
    "    _parse_concat_df,\n",
    "    _extract_parse_smile_df,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we filter out the LLD features that are not relevant and join both LLD/s in a single dataframe\n",
    "# The analysis will be performed in the 0.5.1. Opensmile voicing analysis notebook.eEEEv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse both the compare and emobase LLD\n",
    "df_compare_lld = df_compare_lld.drop(\n",
    "    columns=set(df_compare_lld.columns).difference(\n",
    "        {\n",
    "            \"start\",\n",
    "            # \"end\", Note -> we also drop the end column as both LLD's have a different window size\n",
    "            \"time_str\",\n",
    "            \"pic_name\",\n",
    "            \"DB\",\n",
    "            \"ID\",\n",
    "            # Feature columns\n",
    "            \"voicingFinalUnclipped_sma\",\n",
    "            \"logHNR_sma\",\n",
    "            \"F0final_sma\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "df_emobase_lld = df_emobase_lld.drop(\n",
    "    columns=set(df_emobase_lld.columns).difference(\n",
    "        {\n",
    "            \"start\",\n",
    "            # \"end\",\n",
    "            \"time_str\",\n",
    "            \"pic_name\",\n",
    "            \"DB\",\n",
    "            \"ID\",\n",
    "            \"voiceProb_sma\",\n",
    "            \"pcm_intensity_sma\",\n",
    "            \"pcm_loudness_sma\",\n",
    "            \"F0_sma\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "for c in [\"time_str\"]:\n",
    "    df_emobase_lld[c] = df_emobase_lld[c].astype(\"category\")\n",
    "    df_compare_lld[c] = df_compare_lld[c].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -> these can be removed\n",
    "df_emobase_lld.to_parquet(loc_data_dir / 'voiced_emo_lld.parquet', engine=\"fastparquet\")\n",
    "df_compare_lld.to_parquet(loc_data_dir / 'voiced_comp_lld.parquet', engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -> these can be removed\n",
    "df_emobase_lld = pd.read_parquet(loc_data_dir / 'voiced_emo_lld.parquet', engine=\"fastparquet\")\n",
    "df_compare_lld = pd.read_parquet(loc_data_dir / 'voiced_comp_lld.parquet', engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voiced = pd.merge(\n",
    "    df_compare_lld,\n",
    "    df_emobase_lld,\n",
    "    on=[\"ID\", \"DB\", \"pic_name\", \"time_str\", \"start\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "df_voiced['time_s'] = pd.TimedeltaIndex(df_voiced['start'].astype('str')).total_seconds()\n",
    "df_voiced.to_parquet(loc_data_dir / 'voiced_lld.parquet', engine=\"fastparquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('semi-guided-speech-27YL4uf1-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14accab11a8dde1829a82d5477aee5050c56a20d58c0fbbbf6574e6407175af4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
