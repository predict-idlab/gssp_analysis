{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from sgs_utils.path_conf import loc_data_dir, speech_data_session_dir\n",
    "from sgs_utils.data_filtering import get_valid_mask\n",
    "from sgs_utils.dataframes import groupby_consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session = pd.read_parquet(loc_data_dir.joinpath(\"df_session_tot.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_v = df_session[get_valid_mask(df_session)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_VADs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67da9c8d774943a8908b1159770c0f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not extracted_VADs:\n",
    "    from pyannote.audio import Pipeline\n",
    "\n",
    "    # load the pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\")\n",
    "\n",
    "    # the audio files are processed via pipeline._segementation\n",
    "    df_list = []\n",
    "    for _, r in tqdm(df_session_v.iterrows(), total=len(df_session_v)):\n",
    "        try:\n",
    "\n",
    "            if r.wav_duration_s < 17:\n",
    "                continue\n",
    "\n",
    "            wav_file = list(\n",
    "                speech_data_session_dir.glob(\n",
    "                    f\"*{r.ID}/{r.DB}/{r.pic_name}*{r.time_str}*.wav\"\n",
    "                )\n",
    "            )[0]\n",
    "\n",
    "            out = pipeline._segmentation(wav_file)\n",
    "            probas = out.data.ravel()\n",
    "            sliding_window = out.sliding_window\n",
    "            s = pd.Series(\n",
    "                probas,\n",
    "                index=np.arange(\n",
    "                    start=sliding_window.start + sliding_window.duration,\n",
    "                    step=sliding_window.step,\n",
    "                    stop=sliding_window.start\n",
    "                    + sliding_window.duration\n",
    "                    + sliding_window.step * probas.shape[0],\n",
    "                    dtype=\"float64\",\n",
    "                )[: probas.shape[0]],\n",
    "                name=\"voice_proba\",\n",
    "            )\n",
    "\n",
    "\n",
    "            # slice the series to only retain 15 second data until the penultimate second\n",
    "            # Note: this is the same method as with the \n",
    "            t_end = s.index[-1]\n",
    "            s = s[max(1, t_end - 15 - 1): t_end - 1]\n",
    "\n",
    "            s.index.name = \"time_s\"\n",
    "            s = s.reset_index(drop=False)\n",
    "\n",
    "            s[\"pic_name\"] = wav_file.name.split(\"__\")[0]\n",
    "            s[\"time_str\"] = wav_file.name.split(\"__\")[1].split(\".\")[0]\n",
    "            s[\"DB\"] = wav_file.parent.name\n",
    "            s['sw_duration'] = sliding_window.duration\n",
    "            s['sw_step'] = sliding_window.step\n",
    "            s[\"ID\"] = wav_file.parent.parent.name.split(\"__\")[-1]\n",
    "\n",
    "            df_list.append(s)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    df_vad = pd.concat(df_list)\n",
    "    df_vad['pic_name'] = df_vad['pic_name'].astype('category')\n",
    "    df_vad['DB'] = df_vad['DB'].astype('category')\n",
    "    df_vad['ID'] = df_vad['ID'].astype('category')\n",
    "    df_vad['time_s'] = df_vad['time_s'].astype('float32')\n",
    "\n",
    "    df_vad.to_parquet(loc_data_dir.joinpath('df_vad_fixed_dur.parquet'))\n",
    "else:\n",
    "    df_vad = pd.read_parquet(loc_data_dir.joinpath('df_vad_fixed_dur.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889    3015\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the unique time_str & ID combination\n",
    "ids = df_vad.groupby([\"time_str\", \"ID\"]).size()\n",
    "ids = ids[ids > 0]\n",
    "display(ids.value_counts())\n",
    "ids = ids.reset_index()[['time_str', \"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e5243a9ed247ff8dc2bc4c3d6cdfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_dict = []\n",
    "\n",
    "for _, r in tqdm(ids.iterrows(), total=len(ids)):\n",
    "    mask = (df_vad.ID == r.ID) & (df_vad.time_str == r.time_str)\n",
    "    df_vad_rec = df_vad[mask]\n",
    "    gc = groupby_consecutive(df_vad_rec['voice_proba'] > .5)\n",
    "    gc['duration_s'] = (gc.end - gc.start) * df_vad_rec.sw_duration.iloc[0]\n",
    "\n",
    "    n_speaks = len(gc[gc.voice_proba == True])\n",
    "    n_silences = len(gc[gc.voice_proba == False])\n",
    "\n",
    "    feat_dict.append(\n",
    "        {\n",
    "            \"ID\": r.ID,\n",
    "            \"time_str\": r.time_str,\n",
    "            \"mean_voice_proba\": df_vad_rec['voice_proba'].mean(),\n",
    "            \"speak_max_s\" : gc[gc.voice_proba == True]['duration_s'].max(),\n",
    "            \"speak_mean_s\" : gc[gc.voice_proba == True]['duration_s'].mean(),\n",
    "            \"speak_std_s\": gc[gc.voice_proba == True]['duration_s'].std() if n_speaks > 1 else 0,\n",
    "            \"n_silences\": n_silences,\n",
    "            \"silence_max_s\": gc[gc.voice_proba == False]['duration_s'].max() if n_silences > 0 else 0,\n",
    "            \"silence_mean_s\": gc[gc.voice_proba == False]['duration_s'].mean() if n_silences > 0 else 0,\n",
    "            \"silence_std_s\": gc[gc.voice_proba == False]['duration_s'].std() if n_silences > 1 else 0\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(feat_dict).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.to_parquet(loc_data_dir.joinpath('df_vad_feat.parquet'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('semi-guided-speech-27YL4uf1-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14accab11a8dde1829a82d5477aee5050c56a20d58c0fbbbf6574e6407175af4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
